{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "semi supervised gans"
      ],
      "metadata": {
        "id": "IcExFhI4MYaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# === Load Data ===\n",
        "df = pd.read_csv(\"new_network_train.csv\")\n",
        "X = df.drop(columns=[\"ProtocolName\"]).values\n",
        "y = df[\"ProtocolName\"].astype(np.int64).values\n",
        "\n",
        "# === Preprocessing ===\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=128)\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "latent_dim = 100\n",
        "label_smooth_real = 0.9\n",
        "\n",
        "# === One-hot encoder ===\n",
        "def one_hot(labels, num_classes):\n",
        "    return torch.eye(num_classes, device=labels.device)[labels]\n",
        "\n",
        "# === Focal Loss ===\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logpt = F.log_softmax(input, dim=1)\n",
        "        pt = torch.exp(logpt)\n",
        "        loss = (1 - pt) ** self.gamma * logpt\n",
        "        return F.nll_loss(loss, target)\n",
        "\n",
        "# === Generator ===\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, label_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_dim + label_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        labels = one_hot(labels, num_classes).to(noise.device)\n",
        "        x = torch.cat([noise, labels], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# === Discriminator ===\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, label_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + label_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, data, labels):\n",
        "        labels = one_hot(labels, num_classes).to(data.device)\n",
        "        x = torch.cat([data, labels], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "# === CNN Classifier ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# === Pseudo-labeling function ===\n",
        "def get_confident_pseudo_labels(model, data, threshold=0.9):\n",
        "    model.eval()\n",
        "    data = data.to(device)  # Ensure data is on the same device\n",
        "    with torch.no_grad():\n",
        "        outputs = model(data)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        confidences, predictions = torch.max(probs, dim=1)\n",
        "        mask = confidences >= threshold\n",
        "        confident_data = data[mask]\n",
        "        confident_labels = predictions[mask]\n",
        "    return confident_data.cpu(), confident_labels.cpu()\n",
        "\n",
        "# === Init Models ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = Generator(latent_dim, num_classes, input_dim).to(device)\n",
        "D = Discriminator(input_dim, num_classes).to(device)\n",
        "clf = CNNClassifier(input_dim, num_classes).to(device)\n",
        "\n",
        "# === Losses and Optimizers ===\n",
        "bce = nn.BCELoss()\n",
        "focal_loss = FocalLoss()\n",
        "opt_G = optim.Adam(G.parameters(), lr=0.0002)\n",
        "opt_D = optim.Adam(D.parameters(), lr=0.0002)\n",
        "opt_C = optim.Adam(clf.parameters(), lr=0.001)\n",
        "scheduler_C = optim.lr_scheduler.StepLR(opt_C, step_size=15, gamma=0.5)\n",
        "\n",
        "# === Training Loop ===\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    G.train(); D.train(); clf.train()\n",
        "    total_d_loss, total_g_loss, total_c_loss = 0, 0, 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        batch_size = xb.size(0)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        real_labels = torch.full((batch_size, 1), label_smooth_real).to(device)\n",
        "        fake_labels = torch.zeros((batch_size, 1)).to(device)\n",
        "\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        fake_data = G(z, yb)\n",
        "\n",
        "        D_real = D(xb, yb)\n",
        "        D_fake = D(fake_data.detach(), yb)\n",
        "\n",
        "        loss_D = bce(D_real, real_labels) + bce(D_fake, fake_labels)\n",
        "        opt_D.zero_grad()\n",
        "        loss_D.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        D_fake = D(fake_data, yb)\n",
        "        loss_G = bce(D_fake, real_labels)\n",
        "        opt_G.zero_grad()\n",
        "        loss_G.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # === Train Classifier on real + fake ===\n",
        "        synthetic_data = G(torch.randn(batch_size, latent_dim).to(device), yb)\n",
        "        combined_data = torch.cat([xb, synthetic_data], dim=0)\n",
        "        combined_labels = torch.cat([yb, yb], dim=0)\n",
        "\n",
        "        preds = clf(combined_data)\n",
        "        loss_C = focal_loss(preds, combined_labels)\n",
        "        opt_C.zero_grad()\n",
        "        loss_C.backward()\n",
        "        opt_C.step()\n",
        "\n",
        "        total_d_loss += loss_D.item()\n",
        "        total_g_loss += loss_G.item()\n",
        "        total_c_loss += loss_C.item()\n",
        "\n",
        "    # === Semi-Supervised Learning: Pseudo-labeling ===\n",
        "    pseudo_data, pseudo_labels = get_confident_pseudo_labels(clf, X_test_tensor)\n",
        "    if len(pseudo_data) > 1:\n",
        "        combined_loader = DataLoader(\n",
        "            TensorDataset(torch.cat([X_train_tensor, pseudo_data], dim=0),\n",
        "                          torch.cat([y_train_tensor, pseudo_labels], dim=0)),\n",
        "            batch_size=128, shuffle=True\n",
        "        )\n",
        "\n",
        "        clf.train()\n",
        "        for xb, yb in combined_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = clf(xb)\n",
        "            loss = focal_loss(preds, yb)\n",
        "            opt_C.zero_grad()\n",
        "            loss.backward()\n",
        "            opt_C.step()\n",
        "\n",
        "    scheduler_C.step()\n",
        "    print(f\"[Epoch {epoch+1}] D Loss: {total_d_loss:.4f}, G Loss: {total_g_loss:.4f}, C Loss: {total_c_loss:.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "clf.eval()\n",
        "with torch.no_grad():\n",
        "    preds = clf(X_test_tensor.to(device))\n",
        "    y_pred = preds.argmax(dim=1).cpu().numpy()\n",
        "    y_true = y_test_tensor.cpu().numpy()\n",
        "\n",
        "label_map = {\n",
        "    0: 'AMAZON', 1: 'CLOUDFLARE', 2: 'DROPBOX', 3: 'FACEBOOK',\n",
        "    4: 'GMAIL', 5: 'GOOGLE', 6: 'HTTP', 7: 'HTTP_CONNECT', 8: 'HTTP_PROXY',\n",
        "    9: 'MICROSOFT', 10: 'MSN', 11: 'SKYPE', 12: 'SSL', 13: 'TWITTER',\n",
        "    14: 'WINDOWS_UPDATE', 15: 'YAHOO', 16: 'YOUTUBE'\n",
        "}\n",
        "print(\"\\nâœ… Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nðŸ“Š Classification Report:\\n\", classification_report(y_true, y_pred, target_names=[label_map[i] for i in sorted(label_map)]))\n",
        "\n",
        "# === Save Models ===\n",
        "torch.save(G.state_dict(), \"generator_cgan_ssl.pth\")\n",
        "torch.save(D.state_dict(), \"discriminator_cgan_ssl.pth\")\n",
        "torch.save(clf.state_dict(), \"classifier_cgan_ssl.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuxO3lkJiqAV",
        "outputId": "cdc3b177-9e98-4ed9-887a-2b4c5efa0180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] D Loss: 631.5705, G Loss: 2532.6959, C Loss: 1442.7490\n",
            "[Epoch 2] D Loss: 678.3556, G Loss: 2220.0775, C Loss: 1313.9520\n",
            "[Epoch 3] D Loss: 673.1131, G Loss: 2066.8701, C Loss: 1268.8183\n",
            "[Epoch 4] D Loss: 658.4293, G Loss: 2033.3208, C Loss: 996.4584\n",
            "[Epoch 5] D Loss: 678.4546, G Loss: 1933.0647, C Loss: 659.6484\n",
            "[Epoch 6] D Loss: 718.6039, G Loss: 1792.0583, C Loss: 523.7967\n",
            "[Epoch 7] D Loss: 732.2376, G Loss: 1735.9393, C Loss: 487.0592\n",
            "[Epoch 8] D Loss: 759.5357, G Loss: 1656.9688, C Loss: 468.6067\n",
            "[Epoch 9] D Loss: 795.6981, G Loss: 1556.6133, C Loss: 427.4745\n",
            "[Epoch 10] D Loss: 804.8653, G Loss: 1527.0469, C Loss: 411.1072\n",
            "[Epoch 11] D Loss: 791.7677, G Loss: 1541.9570, C Loss: 401.4506\n",
            "[Epoch 12] D Loss: 807.9213, G Loss: 1528.5791, C Loss: 388.9573\n",
            "[Epoch 13] D Loss: 812.0208, G Loss: 1536.3853, C Loss: 356.9381\n",
            "[Epoch 14] D Loss: 821.2800, G Loss: 1502.9984, C Loss: 335.4334\n",
            "[Epoch 15] D Loss: 830.1162, G Loss: 1477.1575, C Loss: 321.1355\n",
            "[Epoch 16] D Loss: 832.8014, G Loss: 1478.5480, C Loss: 315.5981\n",
            "[Epoch 17] D Loss: 833.2984, G Loss: 1477.7556, C Loss: 300.6043\n",
            "[Epoch 18] D Loss: 836.2629, G Loss: 1468.0831, C Loss: 290.4058\n",
            "[Epoch 19] D Loss: 828.7726, G Loss: 1499.7056, C Loss: 290.8747\n",
            "[Epoch 20] D Loss: 836.3789, G Loss: 1467.1548, C Loss: 277.0264\n",
            "[Epoch 21] D Loss: 834.1071, G Loss: 1483.6659, C Loss: 279.5788\n",
            "[Epoch 22] D Loss: 834.4675, G Loss: 1499.5653, C Loss: 279.1790\n",
            "[Epoch 23] D Loss: 835.9175, G Loss: 1487.8252, C Loss: 264.9756\n",
            "[Epoch 24] D Loss: 836.7941, G Loss: 1495.8054, C Loss: 267.5723\n",
            "[Epoch 25] D Loss: 838.9525, G Loss: 1476.1403, C Loss: 260.1103\n",
            "[Epoch 26] D Loss: 839.6749, G Loss: 1491.3339, C Loss: 250.2935\n",
            "[Epoch 27] D Loss: 842.0345, G Loss: 1486.3084, C Loss: 242.7795\n",
            "[Epoch 28] D Loss: 846.6028, G Loss: 1471.6267, C Loss: 234.2138\n",
            "[Epoch 29] D Loss: 849.8918, G Loss: 1470.8668, C Loss: 228.9559\n",
            "[Epoch 30] D Loss: 851.7601, G Loss: 1462.2727, C Loss: 221.8397\n",
            "[Epoch 31] D Loss: 854.6686, G Loss: 1454.3722, C Loss: 217.1155\n",
            "[Epoch 32] D Loss: 857.2357, G Loss: 1445.9533, C Loss: 210.6554\n",
            "[Epoch 33] D Loss: 855.5025, G Loss: 1449.5068, C Loss: 209.9468\n",
            "[Epoch 34] D Loss: 863.3905, G Loss: 1427.6720, C Loss: 205.3483\n",
            "[Epoch 35] D Loss: 863.4230, G Loss: 1422.8271, C Loss: 202.1433\n",
            "[Epoch 36] D Loss: 867.6470, G Loss: 1414.3593, C Loss: 205.9206\n",
            "[Epoch 37] D Loss: 872.9673, G Loss: 1400.9484, C Loss: 194.2627\n",
            "[Epoch 38] D Loss: 869.4277, G Loss: 1407.4535, C Loss: 195.1768\n",
            "[Epoch 39] D Loss: 877.7745, G Loss: 1375.1036, C Loss: 193.4933\n",
            "[Epoch 40] D Loss: 878.0007, G Loss: 1373.1288, C Loss: 186.4557\n",
            "[Epoch 41] D Loss: 880.3205, G Loss: 1368.9130, C Loss: 188.2565\n",
            "[Epoch 42] D Loss: 883.3514, G Loss: 1358.2574, C Loss: 181.1214\n",
            "[Epoch 43] D Loss: 886.5508, G Loss: 1339.7204, C Loss: 174.9901\n",
            "[Epoch 44] D Loss: 883.7956, G Loss: 1340.7751, C Loss: 176.3201\n",
            "[Epoch 45] D Loss: 887.5163, G Loss: 1332.1580, C Loss: 177.1648\n",
            "[Epoch 46] D Loss: 892.8220, G Loss: 1313.8356, C Loss: 180.0846\n",
            "[Epoch 47] D Loss: 896.9679, G Loss: 1306.1456, C Loss: 170.1152\n",
            "[Epoch 48] D Loss: 896.7119, G Loss: 1295.4319, C Loss: 170.9390\n",
            "[Epoch 49] D Loss: 898.3061, G Loss: 1289.3024, C Loss: 169.0055\n",
            "[Epoch 50] D Loss: 898.5617, G Loss: 1292.9624, C Loss: 160.6442\n",
            "[Epoch 51] D Loss: 901.1784, G Loss: 1280.6801, C Loss: 161.4265\n",
            "[Epoch 52] D Loss: 902.7708, G Loss: 1271.7531, C Loss: 159.0723\n",
            "[Epoch 53] D Loss: 905.8768, G Loss: 1264.3015, C Loss: 160.8998\n",
            "[Epoch 54] D Loss: 906.2429, G Loss: 1262.8436, C Loss: 159.1701\n",
            "[Epoch 55] D Loss: 904.6013, G Loss: 1255.0359, C Loss: 158.8286\n",
            "[Epoch 56] D Loss: 906.2031, G Loss: 1244.1268, C Loss: 159.5927\n",
            "[Epoch 57] D Loss: 908.5125, G Loss: 1247.7696, C Loss: 153.0270\n",
            "[Epoch 58] D Loss: 909.2269, G Loss: 1245.1889, C Loss: 155.2558\n",
            "[Epoch 59] D Loss: 910.0416, G Loss: 1243.9186, C Loss: 149.7014\n",
            "[Epoch 60] D Loss: 910.8213, G Loss: 1231.9081, C Loss: 146.9934\n",
            "[Epoch 61] D Loss: 908.4044, G Loss: 1234.6094, C Loss: 146.3564\n",
            "[Epoch 62] D Loss: 911.9066, G Loss: 1234.3562, C Loss: 156.1316\n",
            "[Epoch 63] D Loss: 912.7211, G Loss: 1227.8237, C Loss: 145.5143\n",
            "[Epoch 64] D Loss: 912.2701, G Loss: 1223.2218, C Loss: 143.1927\n",
            "[Epoch 65] D Loss: 912.1699, G Loss: 1224.1601, C Loss: 143.2469\n",
            "[Epoch 66] D Loss: 915.3757, G Loss: 1217.1614, C Loss: 142.8587\n",
            "[Epoch 67] D Loss: 914.2529, G Loss: 1217.9092, C Loss: 141.1690\n",
            "[Epoch 68] D Loss: 915.9649, G Loss: 1215.0269, C Loss: 140.6679\n",
            "[Epoch 69] D Loss: 914.9000, G Loss: 1210.4014, C Loss: 138.3014\n",
            "[Epoch 70] D Loss: 911.5577, G Loss: 1220.6184, C Loss: 139.3896\n",
            "[Epoch 71] D Loss: 913.6296, G Loss: 1217.0809, C Loss: 134.5941\n",
            "[Epoch 72] D Loss: 914.0117, G Loss: 1214.3018, C Loss: 137.0096\n",
            "[Epoch 73] D Loss: 914.1095, G Loss: 1211.0001, C Loss: 132.8413\n",
            "[Epoch 74] D Loss: 912.7765, G Loss: 1210.7841, C Loss: 138.6038\n",
            "[Epoch 75] D Loss: 909.9649, G Loss: 1213.4144, C Loss: 137.8289\n",
            "[Epoch 76] D Loss: 913.8523, G Loss: 1205.8679, C Loss: 129.3070\n",
            "[Epoch 77] D Loss: 912.9706, G Loss: 1204.2135, C Loss: 130.1640\n",
            "[Epoch 78] D Loss: 910.0342, G Loss: 1205.7342, C Loss: 130.3634\n",
            "[Epoch 79] D Loss: 909.7424, G Loss: 1213.5598, C Loss: 129.4263\n",
            "[Epoch 80] D Loss: 909.3141, G Loss: 1215.4718, C Loss: 127.8865\n",
            "[Epoch 81] D Loss: 909.9959, G Loss: 1210.8262, C Loss: 131.0779\n",
            "[Epoch 82] D Loss: 908.1332, G Loss: 1216.0097, C Loss: 127.8391\n",
            "[Epoch 83] D Loss: 906.4339, G Loss: 1208.4179, C Loss: 127.3207\n",
            "[Epoch 84] D Loss: 905.3545, G Loss: 1212.3289, C Loss: 127.5877\n",
            "[Epoch 85] D Loss: 902.3466, G Loss: 1223.7467, C Loss: 126.5139\n",
            "[Epoch 86] D Loss: 908.5308, G Loss: 1214.9088, C Loss: 125.5631\n",
            "[Epoch 87] D Loss: 903.3998, G Loss: 1216.3818, C Loss: 125.1552\n",
            "[Epoch 88] D Loss: 902.5115, G Loss: 1224.9161, C Loss: 125.4325\n",
            "[Epoch 89] D Loss: 900.1639, G Loss: 1224.8825, C Loss: 123.5218\n",
            "[Epoch 90] D Loss: 901.3415, G Loss: 1226.0026, C Loss: 123.8191\n",
            "[Epoch 91] D Loss: 900.3391, G Loss: 1230.7359, C Loss: 123.0570\n",
            "[Epoch 92] D Loss: 898.6667, G Loss: 1227.4396, C Loss: 123.2091\n",
            "[Epoch 93] D Loss: 896.1914, G Loss: 1232.6930, C Loss: 125.4851\n",
            "[Epoch 94] D Loss: 899.0365, G Loss: 1231.4716, C Loss: 120.9504\n",
            "[Epoch 95] D Loss: 893.9769, G Loss: 1237.1090, C Loss: 122.4338\n",
            "[Epoch 96] D Loss: 897.3807, G Loss: 1236.9600, C Loss: 123.1161\n",
            "[Epoch 97] D Loss: 892.2677, G Loss: 1245.2087, C Loss: 124.4424\n",
            "[Epoch 98] D Loss: 891.8542, G Loss: 1238.6213, C Loss: 124.6771\n",
            "[Epoch 99] D Loss: 893.6900, G Loss: 1245.5406, C Loss: 124.7370\n",
            "[Epoch 100] D Loss: 893.1667, G Loss: 1249.1576, C Loss: 123.8031\n",
            "\n",
            "âœ… Accuracy: 0.9564022578728462\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        AMAZON       1.00      1.00      1.00      1584\n",
            "    CLOUDFLARE       0.99      0.98      0.99      1584\n",
            "       DROPBOX       0.97      0.91      0.94      1584\n",
            "      FACEBOOK       0.97      0.97      0.97      1584\n",
            "         GMAIL       0.98      0.99      0.98      1584\n",
            "        GOOGLE       0.97      0.96      0.97      1584\n",
            "          HTTP       1.00      1.00      1.00      1584\n",
            "  HTTP_CONNECT       0.94      0.92      0.93      1584\n",
            "    HTTP_PROXY       0.92      0.94      0.93      1584\n",
            "     MICROSOFT       0.98      0.99      0.99      1584\n",
            "           MSN       0.87      0.87      0.87      1584\n",
            "         SKYPE       0.93      0.95      0.94      1584\n",
            "           SSL       1.00      1.00      1.00      1584\n",
            "       TWITTER       0.91      0.95      0.93      1584\n",
            "WINDOWS_UPDATE       1.00      1.00      1.00      1584\n",
            "         YAHOO       0.87      0.87      0.87      1584\n",
            "       YOUTUBE       0.97      0.96      0.96      1584\n",
            "\n",
            "      accuracy                           0.96     26928\n",
            "     macro avg       0.96      0.96      0.96     26928\n",
            "  weighted avg       0.96      0.96      0.96     26928\n",
            "\n"
          ]
        }
      ]
    }
  ]
}